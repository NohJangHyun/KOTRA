{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677f2796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58f70fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"호주_세관_v2(new20).csv\")\n",
    "\n",
    "df_title = list(df['title'])\n",
    "df_text = list(df['text'])\n",
    "df_add = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    df_add.append(df_title[i]+\"\"+df_text[i])\n",
    "\n",
    "df_keyword = pd.read_csv(\"호주_번역_100.csv\", index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "898a0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "doc_list = []\n",
    "for doc in df_add:\n",
    "    \n",
    "    #구두점 제거\n",
    "    doc1 = \"\".join([i for i in doc if i not in string.punctuation]).strip()\n",
    "\n",
    "    #숫자 제거\n",
    "    doc2 = \"\".join([i for i in doc1 if not i.isdigit()])\n",
    "    \n",
    "    #월 제거\n",
    "    month = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august',\n",
    "            'september', 'october', 'november', 'december', 'jan', 'feb', 'mar', 'apr',\n",
    "             'may', 'jun','jul', 'aug', 'sep', 'oct', 'nov', 'dec']   \n",
    "\n",
    "    doc3 = \" \".join([i for i in doc2.split() if i not in month])\n",
    "    doc_list.append(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0849989",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_range = (2, 2)\n",
    "stop_words = \"english\"\n",
    "\n",
    "candidates_list = []\n",
    "for doc in doc_list:\n",
    "    count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc3])\n",
    "    candidates = count.get_feature_names_out()\n",
    "    candidates_list.append(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74fa6e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문서 전체와 문서에서 추출한 키워드 수치화\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embeddings = []\n",
    "candidates_embeddings = []\n",
    "for i in range(len(doc_list)):\n",
    "    doc_embeddings.append(model.encode([doc_list[i]])) #전체 문서\n",
    "    candidates_embeddings.append(model.encode(candidates_list[i]))#추출한 bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdf76830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문서와 가장 유사한 키워드 top 20 추출\n",
    "doc_bigram_keywords = []\n",
    "top_n = 20\n",
    "\n",
    "for i in range(len(doc_list)):\n",
    "    doc_bigram_keyword = []\n",
    "    distances = cosine_similarity(doc_embeddings[i], candidates_embeddings[i])\n",
    "    doc_bigram_keyword = [candidates_list[i][index] for index in distances.argsort()[0][-top_n:]]\n",
    "    doc_bigram_keywords.append(doc_bigram_keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df72a7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vehicles australian',\n",
       " 'duty electric',\n",
       " 'belarus currently',\n",
       " 'duty free',\n",
       " 'certain electric',\n",
       " 'hydrogen fuelcell',\n",
       " 'requirement russia',\n",
       " 'revised tariff',\n",
       " 'propose tariff',\n",
       " 'threshold free',\n",
       " 'electric vehicles',\n",
       " 'hybrid vehicles',\n",
       " 'amendments online',\n",
       " 'vehicles hydrogen',\n",
       " 'tariff alterations',\n",
       " 'new tariff',\n",
       " 'car tax',\n",
       " 'russia belarus',\n",
       " 'online tariff',\n",
       " 'tax threshold']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_bigram_keywords[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d428658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문서와 가장 유사한 키워드 벡터화\n",
    "doc_bigram_embeddings = []\n",
    "for i in range(len(doc_list)):\n",
    "    doc_bigram_embedding = []\n",
    "    for keyword in doc_bigram_keywords[i]:\n",
    "        doc_bigram_embedding.append(model.encode(keyword))\n",
    "    doc_bigram_embeddings.append(doc_bigram_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06187fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword : 불러온 호주 키워드 중 번역 Column 추출\n",
    "keyword = list(df_keyword[\"번역\"])\n",
    "keyword_embeddings = []\n",
    "\n",
    "for ele in keyword:\n",
    "    keyword_embeddings.append(model.encode(ele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61d1de9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. check_pairwise_arrays expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(bigram)):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(keyword_embeddings)):\n\u001b[1;32m---> 12\u001b[0m         distances \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbigram\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkeyword_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m distances[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.75\u001b[39m:\n\u001b[0;32m     14\u001b[0m             bigram_result\u001b[38;5;241m.\u001b[39mappend(doc_bigram_keywords[index][i])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KOTRA\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1351\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \n\u001b[0;32m   1319\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;124;03mkernel matrix : ndarray of shape (n_samples_X, n_samples_Y)\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1351\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1353\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KOTRA\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:156\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    147\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    148\u001b[0m         X,\n\u001b[0;32m    149\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    154\u001b[0m     )\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    165\u001b[0m         Y,\n\u001b[0;32m    166\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    171\u001b[0m     )\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KOTRA\\lib\\site-packages\\sklearn\\utils\\validation.py:893\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    890\u001b[0m     )\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m--> 893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    894\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    895\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m    899\u001b[0m     _assert_all_finite(\n\u001b[0;32m    900\u001b[0m         array,\n\u001b[0;32m    901\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    902\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    903\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    904\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. check_pairwise_arrays expected <= 2."
     ]
    }
   ],
   "source": [
    "#문서 키워드(doc_bigram_embedding)와 키워드 100(keyword_embedding) 유사도 비교 \n",
    "bigram_key_result = []\n",
    "keyword_key_result = []\n",
    "\n",
    "\n",
    "for index, bigram in enumerate(doc_bigram_embeddings):\n",
    "    bigram_result = []\n",
    "    key_result=[]\n",
    "    \n",
    "    for i in range(len(bigram)):\n",
    "        for j in range(len(keyword_embeddings)):\n",
    "            distances = cosine_similarity([bigram[i]],[keyword_embeddings[j]])\n",
    "            if distances[0][0] > 0.75:\n",
    "                bigram_result.append(doc_bigram_keywords[index][i])\n",
    "                key_result.append(keyword[j])\n",
    "    bigram_key_result.append(bigram_result)\n",
    "    keyword_key_result.append(key_result)\n",
    "    if index == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bigram_key_result)\n",
    "print(keyword_key_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f033d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = pd.DataFrame()\n",
    "\n",
    "print(bigram_key_result)\n",
    "print(keyword_key_result)\n",
    "\n",
    "# for i in range(len(bigram_key_result)):\n",
    "for i in range(1):\n",
    "    bigram = pd.Series(bigram_key_result[i+1])\n",
    "    keyword = pd.Series(keyword_key_result[i+1])\n",
    "    \n",
    "    df_check['bigram'] = bigram\n",
    "    df_check['keyword'] = keyword\n",
    "\n",
    "df_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ece92e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
